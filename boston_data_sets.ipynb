{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston=load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=pd.DataFrame(boston.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.25179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.570</td>\n",
       "      <td>98.1</td>\n",
       "      <td>3.7979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.57</td>\n",
       "      <td>21.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.965</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>392.53</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.98843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.54</td>\n",
       "      <td>19.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.75026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.924</td>\n",
       "      <td>94.1</td>\n",
       "      <td>4.3996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.33</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.84054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.599</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4.4546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>303.42</td>\n",
       "      <td>16.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.67191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>90.3</td>\n",
       "      <td>4.6820</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.88</td>\n",
       "      <td>14.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.95577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.047</td>\n",
       "      <td>88.8</td>\n",
       "      <td>4.4534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>306.38</td>\n",
       "      <td>17.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.77299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.495</td>\n",
       "      <td>94.4</td>\n",
       "      <td>4.4547</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>387.94</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.674</td>\n",
       "      <td>87.3</td>\n",
       "      <td>4.2390</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.23</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4.87141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.484</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2.3053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.21</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>349.48</td>\n",
       "      <td>24.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>10.23300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.185</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.1705</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>379.70</td>\n",
       "      <td>18.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.229</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>383.32</td>\n",
       "      <td>13.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>5.82401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.242</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.4242</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>5.70818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.750</td>\n",
       "      <td>74.9</td>\n",
       "      <td>3.3317</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>393.07</td>\n",
       "      <td>7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>5.73116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>7.061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.28</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.81838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>5.762</td>\n",
       "      <td>40.3</td>\n",
       "      <td>4.0983</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.92</td>\n",
       "      <td>10.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2.37857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.871</td>\n",
       "      <td>41.9</td>\n",
       "      <td>3.7240</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>370.73</td>\n",
       "      <td>13.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.68</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4.83567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.905</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.1523</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.22</td>\n",
       "      <td>11.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.15086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.454</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>395.09</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>344.05</td>\n",
       "      <td>23.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.707</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.17899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.670</td>\n",
       "      <td>28.8</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>393.29</td>\n",
       "      <td>17.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.28960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.390</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>21.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.26838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.794</td>\n",
       "      <td>70.6</td>\n",
       "      <td>2.8927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.23912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.019</td>\n",
       "      <td>65.3</td>\n",
       "      <td>2.4091</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>395.77</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1      2    3      4      5      6       7     8      9   \\\n",
       "0     0.00632  18.0   2.31  0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n",
       "1     0.02731   0.0   7.07  0.0  0.469  6.421   78.9  4.9671   2.0  242.0   \n",
       "2     0.02729   0.0   7.07  0.0  0.469  7.185   61.1  4.9671   2.0  242.0   \n",
       "3     0.03237   0.0   2.18  0.0  0.458  6.998   45.8  6.0622   3.0  222.0   \n",
       "4     0.06905   0.0   2.18  0.0  0.458  7.147   54.2  6.0622   3.0  222.0   \n",
       "5     0.02985   0.0   2.18  0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "6     0.08829  12.5   7.87  0.0  0.524  6.012   66.6  5.5605   5.0  311.0   \n",
       "7     0.14455  12.5   7.87  0.0  0.524  6.172   96.1  5.9505   5.0  311.0   \n",
       "8     0.21124  12.5   7.87  0.0  0.524  5.631  100.0  6.0821   5.0  311.0   \n",
       "9     0.17004  12.5   7.87  0.0  0.524  6.004   85.9  6.5921   5.0  311.0   \n",
       "10    0.22489  12.5   7.87  0.0  0.524  6.377   94.3  6.3467   5.0  311.0   \n",
       "11    0.11747  12.5   7.87  0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
       "12    0.09378  12.5   7.87  0.0  0.524  5.889   39.0  5.4509   5.0  311.0   \n",
       "13    0.62976   0.0   8.14  0.0  0.538  5.949   61.8  4.7075   4.0  307.0   \n",
       "14    0.63796   0.0   8.14  0.0  0.538  6.096   84.5  4.4619   4.0  307.0   \n",
       "15    0.62739   0.0   8.14  0.0  0.538  5.834   56.5  4.4986   4.0  307.0   \n",
       "16    1.05393   0.0   8.14  0.0  0.538  5.935   29.3  4.4986   4.0  307.0   \n",
       "17    0.78420   0.0   8.14  0.0  0.538  5.990   81.7  4.2579   4.0  307.0   \n",
       "18    0.80271   0.0   8.14  0.0  0.538  5.456   36.6  3.7965   4.0  307.0   \n",
       "19    0.72580   0.0   8.14  0.0  0.538  5.727   69.5  3.7965   4.0  307.0   \n",
       "20    1.25179   0.0   8.14  0.0  0.538  5.570   98.1  3.7979   4.0  307.0   \n",
       "21    0.85204   0.0   8.14  0.0  0.538  5.965   89.2  4.0123   4.0  307.0   \n",
       "22    1.23247   0.0   8.14  0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "23    0.98843   0.0   8.14  0.0  0.538  5.813  100.0  4.0952   4.0  307.0   \n",
       "24    0.75026   0.0   8.14  0.0  0.538  5.924   94.1  4.3996   4.0  307.0   \n",
       "25    0.84054   0.0   8.14  0.0  0.538  5.599   85.7  4.4546   4.0  307.0   \n",
       "26    0.67191   0.0   8.14  0.0  0.538  5.813   90.3  4.6820   4.0  307.0   \n",
       "27    0.95577   0.0   8.14  0.0  0.538  6.047   88.8  4.4534   4.0  307.0   \n",
       "28    0.77299   0.0   8.14  0.0  0.538  6.495   94.4  4.4547   4.0  307.0   \n",
       "29    1.00245   0.0   8.14  0.0  0.538  6.674   87.3  4.2390   4.0  307.0   \n",
       "..        ...   ...    ...  ...    ...    ...    ...     ...   ...    ...   \n",
       "476   4.87141   0.0  18.10  0.0  0.614  6.484   93.6  2.3053  24.0  666.0   \n",
       "477  15.02340   0.0  18.10  0.0  0.614  5.304   97.3  2.1007  24.0  666.0   \n",
       "478  10.23300   0.0  18.10  0.0  0.614  6.185   96.7  2.1705  24.0  666.0   \n",
       "479  14.33370   0.0  18.10  0.0  0.614  6.229   88.0  1.9512  24.0  666.0   \n",
       "480   5.82401   0.0  18.10  0.0  0.532  6.242   64.7  3.4242  24.0  666.0   \n",
       "481   5.70818   0.0  18.10  0.0  0.532  6.750   74.9  3.3317  24.0  666.0   \n",
       "482   5.73116   0.0  18.10  0.0  0.532  7.061   77.0  3.4106  24.0  666.0   \n",
       "483   2.81838   0.0  18.10  0.0  0.532  5.762   40.3  4.0983  24.0  666.0   \n",
       "484   2.37857   0.0  18.10  0.0  0.583  5.871   41.9  3.7240  24.0  666.0   \n",
       "485   3.67367   0.0  18.10  0.0  0.583  6.312   51.9  3.9917  24.0  666.0   \n",
       "486   5.69175   0.0  18.10  0.0  0.583  6.114   79.8  3.5459  24.0  666.0   \n",
       "487   4.83567   0.0  18.10  0.0  0.583  5.905   53.2  3.1523  24.0  666.0   \n",
       "488   0.15086   0.0  27.74  0.0  0.609  5.454   92.7  1.8209   4.0  711.0   \n",
       "489   0.18337   0.0  27.74  0.0  0.609  5.414   98.3  1.7554   4.0  711.0   \n",
       "490   0.20746   0.0  27.74  0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "491   0.10574   0.0  27.74  0.0  0.609  5.983   98.8  1.8681   4.0  711.0   \n",
       "492   0.11132   0.0  27.74  0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "493   0.17331   0.0   9.69  0.0  0.585  5.707   54.0  2.3817   6.0  391.0   \n",
       "494   0.27957   0.0   9.69  0.0  0.585  5.926   42.6  2.3817   6.0  391.0   \n",
       "495   0.17899   0.0   9.69  0.0  0.585  5.670   28.8  2.7986   6.0  391.0   \n",
       "496   0.28960   0.0   9.69  0.0  0.585  5.390   72.9  2.7986   6.0  391.0   \n",
       "497   0.26838   0.0   9.69  0.0  0.585  5.794   70.6  2.8927   6.0  391.0   \n",
       "498   0.23912   0.0   9.69  0.0  0.585  6.019   65.3  2.4091   6.0  391.0   \n",
       "499   0.17783   0.0   9.69  0.0  0.585  5.569   73.5  2.3999   6.0  391.0   \n",
       "500   0.22438   0.0   9.69  0.0  0.585  6.027   79.7  2.4982   6.0  391.0   \n",
       "501   0.06263   0.0  11.93  0.0  0.573  6.593   69.1  2.4786   1.0  273.0   \n",
       "502   0.04527   0.0  11.93  0.0  0.573  6.120   76.7  2.2875   1.0  273.0   \n",
       "503   0.06076   0.0  11.93  0.0  0.573  6.976   91.0  2.1675   1.0  273.0   \n",
       "504   0.10959   0.0  11.93  0.0  0.573  6.794   89.3  2.3889   1.0  273.0   \n",
       "505   0.04741   0.0  11.93  0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "\n",
       "       10      11     12  \n",
       "0    15.3  396.90   4.98  \n",
       "1    17.8  396.90   9.14  \n",
       "2    17.8  392.83   4.03  \n",
       "3    18.7  394.63   2.94  \n",
       "4    18.7  396.90   5.33  \n",
       "5    18.7  394.12   5.21  \n",
       "6    15.2  395.60  12.43  \n",
       "7    15.2  396.90  19.15  \n",
       "8    15.2  386.63  29.93  \n",
       "9    15.2  386.71  17.10  \n",
       "10   15.2  392.52  20.45  \n",
       "11   15.2  396.90  13.27  \n",
       "12   15.2  390.50  15.71  \n",
       "13   21.0  396.90   8.26  \n",
       "14   21.0  380.02  10.26  \n",
       "15   21.0  395.62   8.47  \n",
       "16   21.0  386.85   6.58  \n",
       "17   21.0  386.75  14.67  \n",
       "18   21.0  288.99  11.69  \n",
       "19   21.0  390.95  11.28  \n",
       "20   21.0  376.57  21.02  \n",
       "21   21.0  392.53  13.83  \n",
       "22   21.0  396.90  18.72  \n",
       "23   21.0  394.54  19.88  \n",
       "24   21.0  394.33  16.30  \n",
       "25   21.0  303.42  16.51  \n",
       "26   21.0  376.88  14.81  \n",
       "27   21.0  306.38  17.28  \n",
       "28   21.0  387.94  12.80  \n",
       "29   21.0  380.23  11.98  \n",
       "..    ...     ...    ...  \n",
       "476  20.2  396.21  18.68  \n",
       "477  20.2  349.48  24.91  \n",
       "478  20.2  379.70  18.03  \n",
       "479  20.2  383.32  13.11  \n",
       "480  20.2  396.90  10.74  \n",
       "481  20.2  393.07   7.74  \n",
       "482  20.2  395.28   7.01  \n",
       "483  20.2  392.92  10.42  \n",
       "484  20.2  370.73  13.34  \n",
       "485  20.2  388.62  10.58  \n",
       "486  20.2  392.68  14.98  \n",
       "487  20.2  388.22  11.45  \n",
       "488  20.1  395.09  18.06  \n",
       "489  20.1  344.05  23.97  \n",
       "490  20.1  318.43  29.68  \n",
       "491  20.1  390.11  18.07  \n",
       "492  20.1  396.90  13.35  \n",
       "493  19.2  396.90  12.01  \n",
       "494  19.2  396.90  13.59  \n",
       "495  19.2  393.29  17.60  \n",
       "496  19.2  396.90  21.14  \n",
       "497  19.2  396.90  14.10  \n",
       "498  19.2  396.90  12.92  \n",
       "499  19.2  395.77  15.10  \n",
       "500  19.2  396.90  14.33  \n",
       "501  21.0  391.99   9.67  \n",
       "502  21.0  396.90   9.08  \n",
       "503  21.0  396.90   5.64  \n",
       "504  21.0  393.45   6.48  \n",
       "505  21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=pd.DataFrame(boston.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>21.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>20.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>18.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0    24.0\n",
       "1    21.6\n",
       "2    34.7\n",
       "3    33.4\n",
       "4    36.2\n",
       "5    28.7\n",
       "6    22.9\n",
       "7    27.1\n",
       "8    16.5\n",
       "9    18.9\n",
       "10   15.0\n",
       "11   18.9\n",
       "12   21.7\n",
       "13   20.4\n",
       "14   18.2\n",
       "15   19.9\n",
       "16   23.1\n",
       "17   17.5\n",
       "18   20.2\n",
       "19   18.2\n",
       "20   13.6\n",
       "21   19.6\n",
       "22   15.2\n",
       "23   14.5\n",
       "24   15.6\n",
       "25   13.9\n",
       "26   16.6\n",
       "27   14.8\n",
       "28   18.4\n",
       "29   21.0\n",
       "..    ...\n",
       "476  16.7\n",
       "477  12.0\n",
       "478  14.6\n",
       "479  21.4\n",
       "480  23.0\n",
       "481  23.7\n",
       "482  25.0\n",
       "483  21.8\n",
       "484  20.6\n",
       "485  21.2\n",
       "486  19.1\n",
       "487  20.6\n",
       "488  15.2\n",
       "489   7.0\n",
       "490   8.1\n",
       "491  13.6\n",
       "492  20.1\n",
       "493  21.8\n",
       "494  24.5\n",
       "495  23.1\n",
       "496  19.7\n",
       "497  18.3\n",
       "498  21.2\n",
       "499  17.5\n",
       "500  16.8\n",
       "501  22.4\n",
       "502  20.6\n",
       "503  23.9\n",
       "504  22.0\n",
       "505  11.9\n",
       "\n",
       "[506 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29.00108503],\n",
       "       [36.03180934],\n",
       "       [14.82255381],\n",
       "       [25.03889584],\n",
       "       [18.7617758 ],\n",
       "       [23.25843874],\n",
       "       [17.66106325],\n",
       "       [14.33486047],\n",
       "       [23.01572366],\n",
       "       [20.63232475],\n",
       "       [24.9137504 ],\n",
       "       [18.63823843],\n",
       "       [-6.10459473],\n",
       "       [21.76184921],\n",
       "       [19.24273449],\n",
       "       [26.18610963],\n",
       "       [20.65093389],\n",
       "       [ 5.7909294 ],\n",
       "       [40.49699449],\n",
       "       [17.60897981],\n",
       "       [27.25209766],\n",
       "       [30.06900481],\n",
       "       [11.33334531],\n",
       "       [24.15971925],\n",
       "       [17.85047566],\n",
       "       [15.83640193],\n",
       "       [22.79011378],\n",
       "       [14.52979672],\n",
       "       [22.44033031],\n",
       "       [19.19711348],\n",
       "       [22.43960684],\n",
       "       [25.22165436],\n",
       "       [25.9398993 ],\n",
       "       [17.70193922],\n",
       "       [16.75888473],\n",
       "       [16.93845627],\n",
       "       [31.23575474],\n",
       "       [20.13234607],\n",
       "       [23.76792503],\n",
       "       [24.63508868],\n",
       "       [13.9401346 ],\n",
       "       [32.25447524],\n",
       "       [42.66822383],\n",
       "       [17.33169518],\n",
       "       [27.27926004],\n",
       "       [16.99194395],\n",
       "       [14.06522394],\n",
       "       [25.90637789],\n",
       "       [20.29329067],\n",
       "       [29.95832098],\n",
       "       [21.29199886],\n",
       "       [34.34691795],\n",
       "       [16.0430061 ],\n",
       "       [26.22572769],\n",
       "       [39.53953019],\n",
       "       [22.57770989],\n",
       "       [18.83929963],\n",
       "       [32.72589656],\n",
       "       [25.06908102],\n",
       "       [12.88408119],\n",
       "       [22.67630131],\n",
       "       [30.476643  ],\n",
       "       [31.5279363 ],\n",
       "       [15.89284854],\n",
       "       [20.21760093],\n",
       "       [16.71531932],\n",
       "       [20.51914955],\n",
       "       [25.96660508],\n",
       "       [30.6169449 ],\n",
       "       [11.55223136],\n",
       "       [20.5148055 ],\n",
       "       [27.48404573],\n",
       "       [10.99886484],\n",
       "       [15.67666459],\n",
       "       [23.79906006],\n",
       "       [ 6.20884486],\n",
       "       [21.60533311],\n",
       "       [41.41021597],\n",
       "       [18.76295314],\n",
       "       [ 8.87941928],\n",
       "       [20.83629227],\n",
       "       [13.24493991],\n",
       "       [20.74304185],\n",
       "       [ 9.3643679 ],\n",
       "       [23.22907232],\n",
       "       [31.91529735],\n",
       "       [19.07884047],\n",
       "       [25.51786928],\n",
       "       [29.05011425],\n",
       "       [20.14368288],\n",
       "       [25.58913758],\n",
       "       [ 5.68059194],\n",
       "       [20.0979497 ],\n",
       "       [14.94897476],\n",
       "       [12.5178053 ],\n",
       "       [20.72865134],\n",
       "       [24.74287653],\n",
       "       [-0.19728329],\n",
       "       [13.64405646],\n",
       "       [16.14572268],\n",
       "       [22.27781572],\n",
       "       [24.48435894]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7508837786732915"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6684825753971627"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2246892135692472"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = lm, X = X_train, y = y_train, cv = 20)\n",
    "accuracies.mean()\n",
    "accuracies.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6828646237648383"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold=KFold(n_splits=20,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(KFold(n_splits=20, random_state=7, shuffle=False), dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b105e998480c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \"\"\"\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \"\"\"\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \"\"\"\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[1;32m--> 119\u001b[1;33m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array array(KFold(n_splits=20, random_state=7, shuffle=False), dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "result=cross_val_score(lm,X,y,kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20013786735418707\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearRegression() \n",
    "scoring = 'neg_mean_squared_error' \n",
    "results = cross_val_score(model, X, y, cv=kfold) \n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r=Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=r.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6659608075261689"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6668687223368213"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  ElasticNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e= ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6728074447157107"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6475049964305032"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn=KNeighborsRegressor()\n",
    "knn.fit(X_train,y_train)\n",
    "knn.score(X_test,y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.698596768594845"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decisiom tree regressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr=DecisionTreeRegressor()\n",
    "dtr.fit(X_train,y_train)\n",
    "dtr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.023259347942475417"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr=SVR()\n",
    "svr.fit(X_train,y_train)\n",
    "svr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.linear_model import Lasso \n",
    "from sklearn.linear_model import ElasticNet \n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.svm import SVR \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.ensemble import ExtraTreesRegressor \n",
    "from sklearn.ensemble import AdaBoostRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.25179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.570</td>\n",
       "      <td>98.1</td>\n",
       "      <td>3.7979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.57</td>\n",
       "      <td>21.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.965</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>392.53</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.98843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.54</td>\n",
       "      <td>19.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.75026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.924</td>\n",
       "      <td>94.1</td>\n",
       "      <td>4.3996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.33</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.84054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.599</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4.4546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>303.42</td>\n",
       "      <td>16.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.67191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>90.3</td>\n",
       "      <td>4.6820</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.88</td>\n",
       "      <td>14.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.95577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.047</td>\n",
       "      <td>88.8</td>\n",
       "      <td>4.4534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>306.38</td>\n",
       "      <td>17.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.77299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.495</td>\n",
       "      <td>94.4</td>\n",
       "      <td>4.4547</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>387.94</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.674</td>\n",
       "      <td>87.3</td>\n",
       "      <td>4.2390</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.23</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4.87141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.484</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2.3053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.21</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>349.48</td>\n",
       "      <td>24.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>10.23300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.185</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.1705</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>379.70</td>\n",
       "      <td>18.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.229</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>383.32</td>\n",
       "      <td>13.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>5.82401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.242</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.4242</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>5.70818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.750</td>\n",
       "      <td>74.9</td>\n",
       "      <td>3.3317</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>393.07</td>\n",
       "      <td>7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>5.73116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>7.061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.28</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.81838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>5.762</td>\n",
       "      <td>40.3</td>\n",
       "      <td>4.0983</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.92</td>\n",
       "      <td>10.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2.37857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.871</td>\n",
       "      <td>41.9</td>\n",
       "      <td>3.7240</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>370.73</td>\n",
       "      <td>13.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.68</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4.83567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.905</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.1523</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.22</td>\n",
       "      <td>11.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.15086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.454</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>395.09</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>344.05</td>\n",
       "      <td>23.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.707</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.17899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.670</td>\n",
       "      <td>28.8</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>393.29</td>\n",
       "      <td>17.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.28960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.390</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>21.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.26838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.794</td>\n",
       "      <td>70.6</td>\n",
       "      <td>2.8927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.23912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.019</td>\n",
       "      <td>65.3</td>\n",
       "      <td>2.4091</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>395.77</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1      2    3      4      5      6       7     8      9   \\\n",
       "0     0.00632  18.0   2.31  0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n",
       "1     0.02731   0.0   7.07  0.0  0.469  6.421   78.9  4.9671   2.0  242.0   \n",
       "2     0.02729   0.0   7.07  0.0  0.469  7.185   61.1  4.9671   2.0  242.0   \n",
       "3     0.03237   0.0   2.18  0.0  0.458  6.998   45.8  6.0622   3.0  222.0   \n",
       "4     0.06905   0.0   2.18  0.0  0.458  7.147   54.2  6.0622   3.0  222.0   \n",
       "5     0.02985   0.0   2.18  0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "6     0.08829  12.5   7.87  0.0  0.524  6.012   66.6  5.5605   5.0  311.0   \n",
       "7     0.14455  12.5   7.87  0.0  0.524  6.172   96.1  5.9505   5.0  311.0   \n",
       "8     0.21124  12.5   7.87  0.0  0.524  5.631  100.0  6.0821   5.0  311.0   \n",
       "9     0.17004  12.5   7.87  0.0  0.524  6.004   85.9  6.5921   5.0  311.0   \n",
       "10    0.22489  12.5   7.87  0.0  0.524  6.377   94.3  6.3467   5.0  311.0   \n",
       "11    0.11747  12.5   7.87  0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
       "12    0.09378  12.5   7.87  0.0  0.524  5.889   39.0  5.4509   5.0  311.0   \n",
       "13    0.62976   0.0   8.14  0.0  0.538  5.949   61.8  4.7075   4.0  307.0   \n",
       "14    0.63796   0.0   8.14  0.0  0.538  6.096   84.5  4.4619   4.0  307.0   \n",
       "15    0.62739   0.0   8.14  0.0  0.538  5.834   56.5  4.4986   4.0  307.0   \n",
       "16    1.05393   0.0   8.14  0.0  0.538  5.935   29.3  4.4986   4.0  307.0   \n",
       "17    0.78420   0.0   8.14  0.0  0.538  5.990   81.7  4.2579   4.0  307.0   \n",
       "18    0.80271   0.0   8.14  0.0  0.538  5.456   36.6  3.7965   4.0  307.0   \n",
       "19    0.72580   0.0   8.14  0.0  0.538  5.727   69.5  3.7965   4.0  307.0   \n",
       "20    1.25179   0.0   8.14  0.0  0.538  5.570   98.1  3.7979   4.0  307.0   \n",
       "21    0.85204   0.0   8.14  0.0  0.538  5.965   89.2  4.0123   4.0  307.0   \n",
       "22    1.23247   0.0   8.14  0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "23    0.98843   0.0   8.14  0.0  0.538  5.813  100.0  4.0952   4.0  307.0   \n",
       "24    0.75026   0.0   8.14  0.0  0.538  5.924   94.1  4.3996   4.0  307.0   \n",
       "25    0.84054   0.0   8.14  0.0  0.538  5.599   85.7  4.4546   4.0  307.0   \n",
       "26    0.67191   0.0   8.14  0.0  0.538  5.813   90.3  4.6820   4.0  307.0   \n",
       "27    0.95577   0.0   8.14  0.0  0.538  6.047   88.8  4.4534   4.0  307.0   \n",
       "28    0.77299   0.0   8.14  0.0  0.538  6.495   94.4  4.4547   4.0  307.0   \n",
       "29    1.00245   0.0   8.14  0.0  0.538  6.674   87.3  4.2390   4.0  307.0   \n",
       "..        ...   ...    ...  ...    ...    ...    ...     ...   ...    ...   \n",
       "476   4.87141   0.0  18.10  0.0  0.614  6.484   93.6  2.3053  24.0  666.0   \n",
       "477  15.02340   0.0  18.10  0.0  0.614  5.304   97.3  2.1007  24.0  666.0   \n",
       "478  10.23300   0.0  18.10  0.0  0.614  6.185   96.7  2.1705  24.0  666.0   \n",
       "479  14.33370   0.0  18.10  0.0  0.614  6.229   88.0  1.9512  24.0  666.0   \n",
       "480   5.82401   0.0  18.10  0.0  0.532  6.242   64.7  3.4242  24.0  666.0   \n",
       "481   5.70818   0.0  18.10  0.0  0.532  6.750   74.9  3.3317  24.0  666.0   \n",
       "482   5.73116   0.0  18.10  0.0  0.532  7.061   77.0  3.4106  24.0  666.0   \n",
       "483   2.81838   0.0  18.10  0.0  0.532  5.762   40.3  4.0983  24.0  666.0   \n",
       "484   2.37857   0.0  18.10  0.0  0.583  5.871   41.9  3.7240  24.0  666.0   \n",
       "485   3.67367   0.0  18.10  0.0  0.583  6.312   51.9  3.9917  24.0  666.0   \n",
       "486   5.69175   0.0  18.10  0.0  0.583  6.114   79.8  3.5459  24.0  666.0   \n",
       "487   4.83567   0.0  18.10  0.0  0.583  5.905   53.2  3.1523  24.0  666.0   \n",
       "488   0.15086   0.0  27.74  0.0  0.609  5.454   92.7  1.8209   4.0  711.0   \n",
       "489   0.18337   0.0  27.74  0.0  0.609  5.414   98.3  1.7554   4.0  711.0   \n",
       "490   0.20746   0.0  27.74  0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "491   0.10574   0.0  27.74  0.0  0.609  5.983   98.8  1.8681   4.0  711.0   \n",
       "492   0.11132   0.0  27.74  0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "493   0.17331   0.0   9.69  0.0  0.585  5.707   54.0  2.3817   6.0  391.0   \n",
       "494   0.27957   0.0   9.69  0.0  0.585  5.926   42.6  2.3817   6.0  391.0   \n",
       "495   0.17899   0.0   9.69  0.0  0.585  5.670   28.8  2.7986   6.0  391.0   \n",
       "496   0.28960   0.0   9.69  0.0  0.585  5.390   72.9  2.7986   6.0  391.0   \n",
       "497   0.26838   0.0   9.69  0.0  0.585  5.794   70.6  2.8927   6.0  391.0   \n",
       "498   0.23912   0.0   9.69  0.0  0.585  6.019   65.3  2.4091   6.0  391.0   \n",
       "499   0.17783   0.0   9.69  0.0  0.585  5.569   73.5  2.3999   6.0  391.0   \n",
       "500   0.22438   0.0   9.69  0.0  0.585  6.027   79.7  2.4982   6.0  391.0   \n",
       "501   0.06263   0.0  11.93  0.0  0.573  6.593   69.1  2.4786   1.0  273.0   \n",
       "502   0.04527   0.0  11.93  0.0  0.573  6.120   76.7  2.2875   1.0  273.0   \n",
       "503   0.06076   0.0  11.93  0.0  0.573  6.976   91.0  2.1675   1.0  273.0   \n",
       "504   0.10959   0.0  11.93  0.0  0.573  6.794   89.3  2.3889   1.0  273.0   \n",
       "505   0.04741   0.0  11.93  0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "\n",
       "       10      11     12  \n",
       "0    15.3  396.90   4.98  \n",
       "1    17.8  396.90   9.14  \n",
       "2    17.8  392.83   4.03  \n",
       "3    18.7  394.63   2.94  \n",
       "4    18.7  396.90   5.33  \n",
       "5    18.7  394.12   5.21  \n",
       "6    15.2  395.60  12.43  \n",
       "7    15.2  396.90  19.15  \n",
       "8    15.2  386.63  29.93  \n",
       "9    15.2  386.71  17.10  \n",
       "10   15.2  392.52  20.45  \n",
       "11   15.2  396.90  13.27  \n",
       "12   15.2  390.50  15.71  \n",
       "13   21.0  396.90   8.26  \n",
       "14   21.0  380.02  10.26  \n",
       "15   21.0  395.62   8.47  \n",
       "16   21.0  386.85   6.58  \n",
       "17   21.0  386.75  14.67  \n",
       "18   21.0  288.99  11.69  \n",
       "19   21.0  390.95  11.28  \n",
       "20   21.0  376.57  21.02  \n",
       "21   21.0  392.53  13.83  \n",
       "22   21.0  396.90  18.72  \n",
       "23   21.0  394.54  19.88  \n",
       "24   21.0  394.33  16.30  \n",
       "25   21.0  303.42  16.51  \n",
       "26   21.0  376.88  14.81  \n",
       "27   21.0  306.38  17.28  \n",
       "28   21.0  387.94  12.80  \n",
       "29   21.0  380.23  11.98  \n",
       "..    ...     ...    ...  \n",
       "476  20.2  396.21  18.68  \n",
       "477  20.2  349.48  24.91  \n",
       "478  20.2  379.70  18.03  \n",
       "479  20.2  383.32  13.11  \n",
       "480  20.2  396.90  10.74  \n",
       "481  20.2  393.07   7.74  \n",
       "482  20.2  395.28   7.01  \n",
       "483  20.2  392.92  10.42  \n",
       "484  20.2  370.73  13.34  \n",
       "485  20.2  388.62  10.58  \n",
       "486  20.2  392.68  14.98  \n",
       "487  20.2  388.22  11.45  \n",
       "488  20.1  395.09  18.06  \n",
       "489  20.1  344.05  23.97  \n",
       "490  20.1  318.43  29.68  \n",
       "491  20.1  390.11  18.07  \n",
       "492  20.1  396.90  13.35  \n",
       "493  19.2  396.90  12.01  \n",
       "494  19.2  396.90  13.59  \n",
       "495  19.2  393.29  17.60  \n",
       "496  19.2  396.90  21.14  \n",
       "497  19.2  396.90  14.10  \n",
       "498  19.2  396.90  12.92  \n",
       "499  19.2  395.77  15.10  \n",
       "500  19.2  396.90  14.33  \n",
       "501  21.0  391.99   9.67  \n",
       "502  21.0  396.90   9.08  \n",
       "503  21.0  396.90   5.64  \n",
       "504  21.0  393.45   6.48  \n",
       "505  21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "               12  \n",
       "count  506.000000  \n",
       "mean    12.653063  \n",
       "std      7.141062  \n",
       "min      1.730000  \n",
       "25%      6.950000  \n",
       "50%     11.360000  \n",
       "75%     16.955000  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.199458</td>\n",
       "      <td>0.404471</td>\n",
       "      <td>-0.055295</td>\n",
       "      <td>0.417521</td>\n",
       "      <td>-0.219940</td>\n",
       "      <td>0.350784</td>\n",
       "      <td>-0.377904</td>\n",
       "      <td>0.622029</td>\n",
       "      <td>0.579564</td>\n",
       "      <td>0.288250</td>\n",
       "      <td>-0.377365</td>\n",
       "      <td>0.452220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.199458</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.412995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.404471</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.055295</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.053929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.417521</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.590879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.219940</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.613808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.350784</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.602339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.377904</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.496996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.622029</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>0.488676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.579564</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>0.543993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.288250</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>0.374044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.377365</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.452220</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000 -0.199458  0.404471 -0.055295  0.417521 -0.219940  0.350784   \n",
       "1  -0.199458  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
       "2   0.404471 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
       "3  -0.055295 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
       "4   0.417521 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
       "5  -0.219940  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
       "6   0.350784 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
       "7  -0.377904  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
       "8   0.622029 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
       "9   0.579564 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
       "10  0.288250 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
       "11 -0.377365  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
       "12  0.452220 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
       "\n",
       "          7         8         9         10        11        12  \n",
       "0  -0.377904  0.622029  0.579564  0.288250 -0.377365  0.452220  \n",
       "1   0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  \n",
       "2  -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800  \n",
       "3  -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  \n",
       "4  -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879  \n",
       "5   0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  \n",
       "6  -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339  \n",
       "7   1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  \n",
       "8  -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676  \n",
       "9  -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993  \n",
       "10 -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044  \n",
       "11  0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  \n",
       "12 -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [] \n",
    "models.append(('LR', LinearRegression())) \n",
    "models.append(('LASSO', Lasso())) \n",
    "models.append(('EN', ElasticNet())) \n",
    "models.append(('KNN', KNeighborsRegressor())) \n",
    "models.append(('CART', DecisionTreeRegressor())) \n",
    "models.append(('SVR', SVR()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_folds = 10 \n",
    "seed = 7 \n",
    "scoring = 'neg_mean_squared_error'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: -21.400099 (9.390450)\n",
      "LASSO: -26.431547 (11.655308)\n",
      "EN: -27.504052 (12.309801)\n",
      "KNN: -41.896488 (13.901688)\n",
      "CART: -26.950184 (10.924649)\n",
      "SVR: -85.518342 (31.994798)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "results = [] \n",
    "names = [] \n",
    "for name, model in models: \n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed) \n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring) \n",
    "    results.append(cv_results) \n",
    "    names.append(name) \n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()) \n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEVCAYAAAACW4lMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh7A9fQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14baad94ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure() \n",
    "fig.suptitle('Algorithm Comparison') \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot(results) \n",
    "ax.set_xticklabels(names) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: -21.400099 (9.390450)\n",
      "ScaledLASSO: -26.602585 (8.972576)\n",
      "ScaledEN: -27.944791 (10.570195)\n",
      "ScaledKNN: -20.123490 (12.373834)\n",
      "ScaledCART: -24.467732 (10.236654)\n",
      "ScaledSVR: -29.660002 (16.993025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "pipelines = [] \n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())]))) \n",
    "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\n",
    "pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())]))) \n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())]))) \n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())]))) \n",
    "pipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())]))) \n",
    "results = [] \n",
    "names = [] \n",
    "for name, model in pipelines: \n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed) \n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring) \n",
    "    results.append(cv_results) \n",
    "    names.append(name) \n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()) \n",
    "    print(msg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEVCAYAAADjHF5YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14bad21e860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms \n",
    "fig = plt.figure() \n",
    "fig.suptitle('Scaled Algorithm Comparison') \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot(results) \n",
    "ax.set_xticklabels(names) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train) \n",
    "rescaledX = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_values = np.array([1,3,5,7,9,11,13,15,17,19,21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = dict(n_neighbors=k_values) \n",
    "model = KNeighborsRegressor() \n",
    "kfold = KFold(n_splits=num_folds, random_state=seed) \n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold) \n",
    "grid_result = grid.fit(rescaledX, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -18.133171 using {'n_neighbors': 3}\n",
      "-20.373936 (15.036687) with: {'n_neighbors': 1}\n",
      "-18.133171 (12.958886) with: {'n_neighbors': 3}\n",
      "-20.139550 (12.199280) with: {'n_neighbors': 5}\n",
      "-20.560152 (12.348131) with: {'n_neighbors': 7}\n",
      "-20.382691 (11.617326) with: {'n_neighbors': 9}\n",
      "-21.052064 (11.621184) with: {'n_neighbors': 11}\n",
      "-21.157689 (11.938415) with: {'n_neighbors': 13}\n",
      "-21.538655 (11.532863) with: {'n_neighbors': 15}\n",
      "-22.796125 (11.605158) with: {'n_neighbors': 17}\n",
      "-23.885472 (11.362449) with: {'n_neighbors': 19}\n",
      "-24.364986 (11.920959) with: {'n_neighbors': 21}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "means = grid_result.cv_results_['mean_test_score'] \n",
    "stds = grid_result.cv_results_['std_test_score'] \n",
    "params = grid_result.cv_results_['params'] \n",
    "for mean, stdev, param in zip(means, stds, params): \n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensembles = [] \n",
    "ensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())]))) \n",
    "ensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())]))) \n",
    "ensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor())]))) \n",
    "ensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor())])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledAB: -14.993386 (6.851879)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledGBM: -9.248154 (3.918448)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledRF: -13.269359 (6.551311)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledET: -11.295509 (7.087253)\n"
     ]
    }
   ],
   "source": [
    "results = [] \n",
    "names = [] \n",
    "for name, model in ensembles: \n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed) \n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring) \n",
    "    results.append(cv_results) \n",
    "    names.append(name) \n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()) \n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEVCAYAAADjHF5YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14bad307710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure() \n",
    "fig.suptitle('Scaled Ensemble Algorithm Comparison') \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot(results) \n",
    "ax.set_xticklabels(names) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_train) \n",
    "rescaledX = scaler.transform(X_train) \n",
    "param_grid = dict(n_estimators=np.array([50,100,150,200,250,300,350,400])) \n",
    "model = GradientBoostingRegressor(random_state=seed) \n",
    "kfold = KFold(n_splits=num_folds, random_state=seed) \n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold) \n",
    "grid_result = grid.fit(rescaledX, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbaz\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=400, presort='auto', random_state=7,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_train) \n",
    "rescaledX = scaler.transform(X_train) \n",
    "model = GradientBoostingRegressor(random_state=seed, n_estimators=400) \n",
    "model.fit(rescaledX, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
